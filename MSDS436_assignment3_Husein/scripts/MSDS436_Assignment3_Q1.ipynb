{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment on Spark\n",
    "\n",
    "One of the most common uses of Spark is analyzing and processing log files. In this assignment, we will put Spark to good use for an OSS project that retrieves and downloads data from GitHub, called GHTorrent. GHTorrent works by following the Github event timeline and then retrieving all items linked from each event recursively and exhaustively. To make monitoring and debugging easier, the GHTorrent maintainers use extensive runtime logging for the downloader scripts.\n",
    "\n",
    "An extract of what the GHTorrent log looks like:\n",
    "DEBUG, 2017-03-23T10:02:27+00:00, ghtorrent-40 -- ghtorrent.rb: Repo EFForg/https-everywhere exists\n",
    "DEBUG, 2017-03-24T12:06:23+00:00, ghtorrent-49 -- ghtorrent.rb: Repo Shikanime/print exists\n",
    "INFO, 2017-03-23T13:00:55+00:00, ghtorrent-42 -- api_client.rb: Successful request. URL: https://api.github.com/repos/CanonicalLtd/maas-docs/issues/365/events?per_page=100, Remaining: 4943, Total: 88 ms\n",
    "WARN, 2017-03-23T20:04:28+00:00, ghtorrent-13 -- api_client.rb: Failed request. URL: https://api.github.com/repos/greatfakeman/Tabchi/commits?sha=Tabchi&per_page=100, Status code: 404, Status: Not Found, Access: ac6168f8776, IP: 0.0.0.0, Remaining: 3031\n",
    "DEBUG, 2017-03-23T09:06:09+00:00, ghtorrent-2 -- ghtorrent.rb: Transaction committed (11 ms)\n",
    "\n",
    "Each log line comprises of a standard part (up to .rb:) and an operation-specific part. The standard part fields are like so:\n",
    "Logging level, one of DEBUG, INFO, WARN, ERROR (separated by ,)\n",
    "A timestamp (separated by ,)\n",
    "The downloader id, denoting the downloader instance (separated by --)\n",
    "The retrieval stage, denoted by the Ruby class name, one of:\n",
    "event_processing\n",
    "ght_data_retrieval\n",
    "api_client\n",
    "retriever\n",
    "ghtorrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets ignore warnings for now\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hag8665\\Desktop\\MSDS 436\\Assignment3\\data\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\hag8665\\Desktop\\MSDS 436\\Assignment3\\data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hag8665\\\\Anaconda3\\\\envs\\\\new_environment\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hag8665\\Anaconda3\\envs\\new_environment\\lib\\site-packages\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "\n",
    "# Initialize Spark Context\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Assignment3\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and parsing the file\n",
    "# for parsing rdd rows\n",
    "# Columns:\n",
    "# 0: logging level, 1: timestamp, 2: downloader id, \n",
    "# 3: retrieval stage, 4: Action?\n",
    "def myParse(line):\n",
    "    line = line.replace(' -- ', ', ')\n",
    "    line = line.replace('.rb: ', ', ')\n",
    "    line = line.replace(', ghtorrent-', ', ')\n",
    "    return line.split(', ', 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRDD(filename):\n",
    "    textFile = sc.textFile(\"ghtorrent-logs.txt\")\n",
    "    parsedRDD = textFile.map(myParse)\n",
    "    return parsedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowrdd = getRDD(\"ghtorrent-logs.txt\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9669788"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many lines does the RDD contain?\n",
    "\n",
    "rowrdd.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132158\n"
     ]
    }
   ],
   "source": [
    "# Count the number of WARNing messages\n",
    "countwarns = rowrdd.filter(lambda x: x[0] == \"WARN\")\n",
    "print(countwarns.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many repositories where processed in total? Use the api_client lines only.\n",
    "\n",
    "import itertools\n",
    "# Add repositories as column 5\n",
    "\n",
    "# rewrite with split, and use only api_client\n",
    "def parseRepos(x):\n",
    "    try:\n",
    "        # Filter for repos by looking for it in url\n",
    "        # For instance:\n",
    "        # Successful request. URL: https://api.github.com/repos/CanonicalLtd/maas-docs\n",
    "        # /issues/365/events?per_page=100, Remaining: 4943, Total: 88 ms\n",
    "        # Should return \"CanonicalLtd/maas-docs/maas-docs\"\n",
    "        split = x[4].split('/')[4:6]\n",
    "        joinedSplit = '/'.join(split)\n",
    "        result = joinedSplit.split('?')[0]\n",
    "    except: \n",
    "        result = ''\n",
    "    x.append(result)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Filters out rows without enough elements (about 50 rows)\n",
    "filteredRdd = rowrdd.filter(lambda x: len(x) == 5) \n",
    "\n",
    "# Only look at api_client calls\n",
    "apiRdd = filteredRdd.filter(lambda x: x[3] == \"api_client\")\n",
    "\n",
    "# Add another column with the repo if can find one, otherwise ''\n",
    "reposRdd = apiRdd.map(parseRepos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78588\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows without repo\n",
    "removedEmpty = reposRdd.filter(lambda x: x[5] != '')\n",
    "\n",
    "# Group by repo and count\n",
    "uniqueRepos = removedEmpty.groupBy(lambda x: x[5])\n",
    "\n",
    "# How Many UniqueRepo do we have ?\n",
    "print(uniqueRepos.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INSIGHTS & ANALYTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('13', 135978)\n"
     ]
    }
   ],
   "source": [
    "# Which client did most HTTP requests?\n",
    "# Group by, count and find max\n",
    "\n",
    "usersHttp = apiRdd.groupBy(lambda x: x[2])\n",
    "usersHttpSum = usersHttp.map(lambda x: (x[0], x[1].__len__()))\n",
    "print(usersHttpSum.max(key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('13', 79623)\n"
     ]
    }
   ],
   "source": [
    "# Which client did most FAILED HTTP requests? Use group_by to provide an answer.\n",
    "# filter failed http requests\n",
    "onlyFailed = apiRdd.filter(lambda x: x[4].split(' ', 1)[0] == \"Failed\")\n",
    "\n",
    "# Group by, count, find max\n",
    "usersFailedHttp = onlyFailed.groupBy(lambda x: x[2])\n",
    "usersFailedHttpSum = usersFailedHttp.map(lambda x: (x[0], x[1].__len__()))\n",
    "print(usersFailedHttpSum.max(key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10', 2662487)\n"
     ]
    }
   ],
   "source": [
    "# What is the most active hour of day?\n",
    "# Get hour of the day from timestamp and add it\n",
    "def appendAndReturn(x, toAdd):\n",
    "    x.append(toAdd)\n",
    "    return x\n",
    "\n",
    "# Split date to hour only\n",
    "onlyHours = filteredRdd.map(lambda x: appendAndReturn(x, x[1].split('T', 1)[1].split(':', 1)[0]))\n",
    "\n",
    "# Group by, count, find max\n",
    "groupOnlyHours = onlyHours.groupBy(lambda x: x[5])\n",
    "hoursCount = groupOnlyHours.map(lambda x: (x[0], x[1].__len__()))\n",
    "print(hoursCount.max(key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('greatfakeman/Tabchi', 79524)\n"
     ]
    }
   ],
   "source": [
    "# What is the most active repository (hint: use messages from the ghtorrent.rb layer only)?\n",
    "\n",
    "# Group by, count, find max\n",
    "activityRepos = removedEmpty.groupBy(lambda x: x[5])\n",
    "countActivityRepos = activityRepos.map(lambda x: (x[0], x[1].__len__()))\n",
    "print(countActivityRepos.max(key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ac6168f8776', 79623)\n"
     ]
    }
   ],
   "source": [
    "# Which access keys are failing most often? (hint: extract the Access: ... part from failing requests)?\n",
    "# Add access code\n",
    "addedCodes = onlyFailed.map(lambda x: appendAndReturn(x, x[4].split('Access: ', 1)[1].split(',', 1)[0]))\n",
    "\n",
    "# most failed access\n",
    "\n",
    "accessCodes = addedCodes.groupBy(lambda x: x[5])\n",
    "countAccessCodes = accessCodes.map(lambda x: (x[0], x[1].__len__()))\n",
    "print(countAccessCodes.max(key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide 2 more insights relating to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ght_data_retrieval : 335178\n",
      "ghtorrent : 5703903\n",
      "api_client : 1299697\n",
      "event_processing : 12724\n",
      "geolocator : 538\n",
      "vhost=/ : 97\n",
      "retriever : 2317594\n"
     ]
    }
   ],
   "source": [
    "# count the the distinct request type \n",
    "getdistinct =filteredRdd.filter(lambda x : x[3]!=\"\")\n",
    "getdistpair = getdistinct.map(lambda word:(word[3],1))\n",
    "getdistcount = getdistpair.reduceByKey(lambda x,y :x+y)\n",
    "\n",
    "for word,count in getdistcount.collect():\n",
    "    print(\"{} : {}\".format(word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('4', 18694)\n"
     ]
    }
   ],
   "source": [
    "# Which client did most scuessfull HTTP requests? Use group_by to provide an answer.\n",
    "# filter failed http requests\n",
    "onlySuccess = apiRdd.filter(lambda x: x[4].split(' ', 1)[0] == \"Successful\")\n",
    "\n",
    "# Group by, count, find max\n",
    "usersSuccessHttp = onlySuccess.groupBy(lambda x: x[2])\n",
    "usersSuccessHttpSum = usersSuccessHttp.map(lambda x: (x[0], x[1].__len__()))\n",
    "print(usersSuccessHttpSum.max(key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the important-repos.csv file to an RDD (let's call it interesting). How many records are there?\n",
    "textfile = sc.textFile(\"important-repos.csv\")\n",
    "interesting = textfile.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1436"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many records in the log file refer to entries in the interesting file?\n",
    "interesting.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeRepo(x):\n",
    "    try:\n",
    "        x[5] = x[5].split(\"/\")[1]\n",
    "    except:\n",
    "        x[5] = ''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "interestingRepo = interesting.keyBy(lambda x: x[3])\n",
    "logLineRepo = reposRdd.map(changeRepo).filter(lambda x: x[5] != '').keyBy(lambda x: x[5])\n",
    "\n",
    "joinedRepo = interestingRepo.join(logLineRepo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello-world', 740), ('test', 309), ('demo', 166)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which of the interesting repositories has the most failed API calls?\n",
    "\n",
    "joinedRepo.filter(lambda v: v[1][1][4].startswith(\"Failed\")) \\\n",
    ".map(lambda key: (key[0], 1)) \\\n",
    ".reduceByKey(lambda a, b: a + b) \\\n",
    ".sortBy(lambda  v: v[1], False) \\\n",
    ".take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the interesting repos file using Spark's CSV parser. Convert the log RDD to a Dataframe.\n",
    "interesting_df = spark.read \\\n",
    "                        .format(\"csv\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .option(\"inferSchema\", \"true\") \\\n",
    "                        .load(\"important-repos.csv\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = reposRdd.map(changeRepo).filter(lambda x: x[5] != '').toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1435"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interesting_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87930"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat all 3 queries in the \"Joining\" section above using either SQL or the Dataframe API. \n",
    "# Measure the time it takes to execute them.\n",
    "joined_df = interesting_df.join(log_df, interesting_df.name == log_df._6);\n",
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
